{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0214| CEF, major update and put in sync with under/overfitting exe.         \n",
    "2018-0220| CEF, fixed revision table malformatting.\n",
    "2018-0225| CEF, minor text updates, and made Qc optional.\n",
    "2018-0225| CEF, updated code, made more functions.\n",
    "2018-0311| CEF, corrected RSME to RMSE.\n",
    "2019-1008| CEF, updated to ITMAL E19.\n",
    "2020-0314| CEF, updated to ITMAL F20.\n",
    "2020-1015| CEF, updated to ITMAL E20.\n",
    "\n",
    "## Generalization Error\n",
    "\n",
    "In this exercise, we need to explain all important overall concepts in training. Let's begin with Figure 5.3 from Deep Learning, Ian Goodfellow, et. al. [DL], that pretty much sums it all up\n",
    "\n",
    "<img src=\"https://blackboard.au.dk/bbcswebdav/courses/BB-Cou-UUVA-91831/Fildeling/L07/Figs/dl_generalization_error.png\" alt=\"WARNING: you need to be logged into Blackboard to view images\" style=\"height:500px\">\n",
    "\n",
    "### Qa On Generalization Error\n",
    "\n",
    "Write a detailed description of figure 5.3 (above) for your hand-in.\n",
    " \n",
    "All concepts in the figure must be explained \n",
    "\n",
    "* training/generalization error, \n",
    "* underfit/overfit zone, \n",
    "* optimal capacity, \n",
    "* generalization gab, \n",
    "* and the two axes: x/capacity, y/error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ...in text\n",
    "assert False, \"TODO: write some text..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb A MSE-Epoch/Error Plot\n",
    "\n",
    "Next, we look at a SGD model for fitting polynomial, that is _polynomial regression_ similar to what GÃ©ron describes in [HOML] (\"Polynomial Regression\" + \"Learning Curves\"). \n",
    "\n",
    "Review the code below for plotting the RMSE vs. the iteration number or epoch below (three cells, part I/II/III). \n",
    "\n",
    "Write a short description of the code, and comment on the important points in the generation of the (R)MSE array.\n",
    "\n",
    "The training phase output lots of lines like \n",
    "\n",
    "> `epoch= 104, mse_train=1.50, mse_val=2.37` <br>\n",
    "> `epoch= 105, mse_train=1.49, mse_val=2.35`\n",
    "\n",
    "What is an ___epoch___ and what is `mse_train` and `mse_val`?\n",
    "\n",
    "NOTE: the generalization plot figure 5.3 in [DL] (above) and the plots below have different x-axis, and are not to be compared directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part I)\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def GenerateData():\n",
    "    m = 100\n",
    "    X = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split( \\\n",
    "        X[:50], y[:50].ravel(), \\\n",
    "        test_size=0.5, \\\n",
    "        random_state=10)\n",
    "\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"X_val  .shape=\",X_val.shape)\n",
    "print(\"y_train.shape=\",y_train.shape)\n",
    "print(\"y_val  .shape=\",y_val.shape)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled   = poly_scaler.transform(X_val)\n",
    "\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, \"b.\", label=\"All X-y Data\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18, )\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n",
    "\n",
    "print('OK')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part II)\n",
    "\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(\"Training...n_epochs=\",n_epochs)\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    sgd_reg = SGDRegressor(max_iter=1,\n",
    "                           penalty=None,\n",
    "                           eta0=0.0005,\n",
    "                           warm_start=True,\n",
    "                           learning_rate=\"constant\",\n",
    "                           tol=-float(\"inf\"),\n",
    "                           random_state=42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict   = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train=mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val  =mean_squared_error(y_val  , y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors  .append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"  epoch={epoch:4d}, mse_train={mse_train:4.2f}, mse_val={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = Train(X_train_poly_scaled, y_train, X_val_poly_scaled, y_val, n_epochs, True)\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part III)\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(train_errors), \"b--\", linewidth=2, label=\"Training set\")\n",
    "plt.plot(np.sqrt(val_errors), \"g-\", linewidth=3, label=\"Validation set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code review..\n",
    "assert False, \"TODO: code review in text form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc  Early Stopping\n",
    "\n",
    "How would you implement ___early stopping___, in the code above? \n",
    "\n",
    "Try it out in the code, and write an explanation of the early stopping concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: early stopping..\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(\"Training...n_epochs=\",n_epochs)\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    sgd_reg = SGDRegressor(max_iter=1,\n",
    "                           penalty=None,\n",
    "                           eta0=0.0005,\n",
    "                           warm_start=True,\n",
    "                           learning_rate=\"constant\",\n",
    "                           tol=-float(\"inf\"),\n",
    "                           random_state=42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict   = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train=mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val  =mean_squared_error(y_val  , y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors  .append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"  epoch={epoch:4d}, mse_train={mse_train:4.2f}, mse_val={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Explain the Polynomial RMSE-Capacity plot\n",
    "\n",
    "Now we revisit the concepts from `capacity_under_overfitting.ipynb` notebook and the polynomial fitting with a given capacity (polynomial degree).\n",
    "\n",
    "Peek into the cell below (code similar to what we saw in `capacity_under_overfitting.ipynb`), and explain the generated RMSE-Capacity plot. Why does the _training error_ keep dropping, while the _CV-error_ drops until around capacity 3, and then begin to rise again?\n",
    "\n",
    "What does the x-axis _Capacity_ and y-axis _RMSE_ represent?\n",
    "\n",
    "Try increasing the model capacity. What happens when you do plots for `degrees` larger than around 10? Relate this with what you found via Qc in `capacity_under_overfitting.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and review this code\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData():\n",
    "    n_samples = 30\n",
    "    #degrees = [1, 4, 15]\n",
    "    degrees = range(1,8)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y, degrees\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, degrees  = GenerateData()\n",
    "\n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "capacities, rmses_training, rmses_cv= [], [], []\n",
    "for i in range(len(degrees)):\n",
    "    d=degrees[i]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    \n",
    "    Z = X[:, np.newaxis]\n",
    "    pipeline.fit(Z, y)\n",
    "    \n",
    "    p = pipeline.predict(Z)\n",
    "    train_rms = mean_squared_error(y,p)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, Z, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = -scores.mean()\n",
    "    \n",
    "    rmse_training=sqrt(train_rms)\n",
    "    rmse_cv=sqrt(score_mean)\n",
    "    \n",
    "    print(f\"  degree={d:4d}, rmse_training={rmse_training:4.2f}, rmse_cv={rmse_cv:4.2f}\")\n",
    "    \n",
    "    capacities    .append(d)\n",
    "    rmses_training.append(rmse_training)\n",
    "    rmses_cv      .append(rmse_cv)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(capacities, rmses_training, \"b--\", linewidth=2, label=\"training RMSE\")\n",
    "plt.plot(capacities, rmses_cv, \"g-\", linewidth=2, label=\"CV RMSE\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Capacity\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate..\n",
    "assert False, \"TODO: ...answer in text form\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
