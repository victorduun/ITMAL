# Setup

1. Installer chocolatey

Kør følgende kommando i powershell som  adminstrator:
>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))

2. Installer make:

>choco install make

3. Installer kaggle 


>make kaggle

4. Hent / slet dataset vha. makefile kommandoer.

>make all 


# Mappestruktur

```
project
│   README.md
│   makefile
└───data
│   │
│   └───external    <- Third party data
│   └───interim     <- Transformed intermediate data, not ready for modelling
│   └───processed   <-Prepared data, ready for modelling
│   └───raw         <- Immutable original data
│   
└───models          <- Serialized model
└───notebooks       <-Jupyter notebooks for exploration, communication and prototyping
└───src             <-Folder containing project source code
│   └───data        <- Folder containing scripts to download/generate data
│   └───features    <- Folder containing scripts to transform data for modelling
│   └───model       <- Folder containing scripts to train and predict
```

# Data
*External:* This is data extracted from third party sources (Immutable data). If no third party data is extracted then this folder is obsolete.

*Interim:* In the event external data being available, this data would be the data that we would load for feature engineering by using a script in the src/data directory. This dataset is generated by performing various joins and/or merges to combine the external and raw data.

*Processed:* This is the data that has been transformed using various machine learning techniques. The features folder that we will get to in the src folder performs various transformations on the data to allow it to be ready for modelling. It serves as a good idea to persist the processed data in order to shorten the training time of our model.

*Raw:* Having a local subset copy of data ensures that you have a static dataset to perform task on. Additionally, this overcomes any workflow breakdowns due to network latency issues. This data should be considered immutable. If there is no external data then this is the data to be downloaded by the script in src\data.
# Models
We use a script in src\models for training of our Machine Learning model. We may need to restore or reuse the model with other models to build an ensemble or to compare and we may decide upon a model that we want to deploy. In order to do this we save the trained model to a file (usually a pickle format) and that file would be saved in this directory.

# Notebooks

Jupyter notebooks are excellent for prototyping, exploring and communicating findings, however they aren’t very good for long-term growth and can be less effective for reproducibility. Notebooks can be further divided into sub-folders such as Notebooks\explorations and Notebooks\PoC . Using a good naming conventions helps to distinguish what populates each notebook — a useful template is ```<step>-<user>-<description>.ipynb``` (i.e. 01-kpy-eda.ipynb) where the step serves as an ordering mechanism, the creator’s first name initial, and first 2 letters of surname and description of what the notebook contains.

# Src

*Data:* In this directory we have the scripts that ingest the data from wherever it is being generated and transform that data so that it is in a state that further feature engineering can take place.

*Features:* In this directory, we have a script that manipulates the data and puts it in a format that can be consumed by our machine learning model.

*Models:* Contains scripts that are used to build and train our model.

# References

https://towardsdatascience.com/structuring-machine-learning-projects-be473775a1b6
https://towardsdatascience.com/structure-and-automated-workflow-for-a-machine-learning-project-2fa30d661c1e