{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0301| CEF, initial.\n",
    "2018-0306| CEF, updated.\n",
    "2018-0307| CEF, split Qb into Qb+c+d and added NN comment.\n",
    "2018-0311| CEF, updated Qa and $w_0$ issues.\n",
    "2018-0311| CEF, updated Qd with plot and Q.\n",
    "2018-0311| CEF, clarified $w_0$ issue and update $\\tilde{J}$'s.\n",
    "2019-1015| CEF, updated for ITMAL E19.\n",
    "2019-1019| CEF, updated text, added float-check functions.\n",
    "2020-0323| CEF, updated to ITMAL F20.\n",
    "2020-1020| CEF, updated to ITMAL E20.\n",
    "2020-1027| CEF, minor updates.\n",
    "2020-1028| CEF, made preprocessing optional part of Qq (tug-of-war).\n",
    "\n",
    "## Regulizers\n",
    "\n",
    "### Resume of The Linear Regressor\n",
    "\n",
    "For our  data set $\\mathbf{X}$ and target $\\mathbf{y}$ \n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\pred{_{\\scriptsize\\mbox{pred}}}\n",
    "    \\def\\bM{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\newcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\newcommand\\dfrac[2]{\\frac{\\mbox{d}~#1}{\\mbox{d}#2}}\n",
    "\\bX =\n",
    "    \\ac{cccc}{\n",
    "        x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "        x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "        \\vdots      &             &        & \\vdots \\\\\n",
    "        x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "    }\n",
    ", ~~~~~~~~\n",
    "\\by =\n",
    "    \\ac{c}{\n",
    "         y\\pown{1} \\\\\n",
    "         y\\pown{2} \\\\\n",
    "         \\vdots \\\\\n",
    "         y\\pown{n} \\\\\n",
    "    }\n",
    "%, ~~~~~~~~\n",
    "%\\bx\\powni = \n",
    "%    \\ac{c}{\n",
    "%        1\\\\\n",
    "%        x_1\\powni \\\\\n",
    "%        x_2\\powni \\\\ \n",
    "%        \\vdots \\\\\n",
    "%        x_d\\powni\n",
    "%     }  \n",
    "$$\n",
    "\n",
    "a __linear regressor__ model, with the $d$-dimensional (expressed here withoutthe bias term, $w_0$) weight column vector,\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_1 \\\\\n",
    "         w_2 \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "\n",
    "was previously found to be of the form\n",
    "\n",
    "$$\n",
    "    y\\powni\\pred =  \\bw^\\top \\bx\\powni\n",
    "$$\n",
    "\n",
    "for a single data instance, or for the full data set in a compact matrix notation \n",
    "\n",
    "$$\n",
    "    \\by\\pred = \\bX \\bw\n",
    "$$\n",
    "\n",
    "(and rememering to add the bias term $w_0$ on $\\bw$ and correspondingly adding fixed '1'-column in the $\\bX$ matrix, later.) \n",
    "\n",
    "An accociated cost function could be the MSE \n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\mbox{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                            &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\pred \\right)^2\\\\\n",
    "                            &\\propto ||\\bX \\bw - \\by\\pred||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "here using the squared Euclidean norm, $\\norm{2}^2$, via the $||\\cdot||_2^2$ expressions. We used the MSE to express the total cost function, $J$, as\n",
    "\n",
    "$$\n",
    "   \\mbox{MSE} \\propto J = ||\\bX \\bw - \\by\\pred||_2^2\n",
    "$$\n",
    "\n",
    "give or take a few constants, like $1/2$ or $1/n$.\n",
    "\n",
    "### Adding Regularization to the Linear Regressor\n",
    "\n",
    "Now the weights, $\\bw$ (previously also known as $\\btheta$), in this model are free to take on any value they like, and this can  lead to both numerical problems and overfitting, if the algorithm decides to drive the weights to insane, humongous values, say $10^{200}$ or similar.\n",
    "\n",
    "Also for some models, neural networks in particular, having weights outside the range -1 to 1 (or 0 to 1) may cause complete saturation of some of the internal non-linear components (the activation function). \n",
    "\n",
    "Now, enters the ___regularization___ of the model: keep the weights at a sane level while doing the numerical gradient descent (GD) in the search space. This can quite simply be done by adding a ___penalty___ part, $\\Omega$, to the $J$ function as\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\tilde{J} &= J + \\alpha \\Omega(\\bw)\\\\\n",
    "                  &= \\frac{1}{n} ||\\bX \\bw - \\by||_2^2 + \\alpha ||\\bw||^2_2\n",
    "     }\n",
    "$$\n",
    "\n",
    "So, the algorithm now has to find an optimal value (minimum of $J$) for both the usual MSE part and for the added penalty scaled with the $\\alpha$ constant.\n",
    "\n",
    "### Regularization and Optimization for Neural Networks (NNs)\n",
    "\n",
    "The regularization method mentioned here is strictly for a linear regression model, but such a model constitutes a major part of the neurons (or perceptrons), used in neural networks. \n",
    "\n",
    "### Qa The Penalty Factor\n",
    "\n",
    "Now, lets examine  what $||\\bw||^2_2$ effectively mean? It is composed of our well-known $\\norm{2}^2$ norm and can also be expressed as simple as\n",
    "\n",
    "$$\n",
    "  ||\\bw||^2_2 = \\bw^\\top\\bw\n",
    "$$\n",
    "\n",
    "Construct a penaltiy function that implements $\\bw^\\top\\bw$, re-using any functions from `numpy` (implementation could be a tiny _one-liner_).\n",
    "\n",
    "Take $w_0$ into account, this weight factor should NOT be included in the norm. Also checkup on `numpy`s `dot` implementation, if you have not done so already: it is a typical pythonic _combo_ function, doing both dot op's (inner product) and matrix multiplication (outer product) dependent on the shape of the input parameters.\n",
    "\n",
    "Then run it on the three test vectors below, and explain when the penalty factor is low and when it is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(setup..)\n"
     ]
    }
   ],
   "source": [
    "# Qa..first define some numeric helper functions for the test-vectors..\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def isFloat(x):\n",
    "    # is there a python single/double float??\n",
    "    return isinstance(x, float) or isinstance(x, np.float32) or isinstance(x, np.float64)\n",
    "    # NOT defined on Windows?:   or isinstance(x, np.float128)      \n",
    "\n",
    "# Checks that a 'float' is 'sane' (original from libitmal)\n",
    "def CheckFloat(x, checkrange=False, xmin=1E-200, xmax=1E200, verbose=0):\n",
    "    if verbose>1:\n",
    "        print(f\"CheckFloat({x}, type={type(x)}\")\n",
    "    if isinstance(x, collections.Iterable):\n",
    "        for i in x:\n",
    "            CheckFloat(i, checkrange=checkrange, xmin=xmin, xmax=xmax, verbose=verbose)\n",
    "    else:\n",
    "        #if (isinstance(x,int)):\n",
    "        #    print(\"you gave me an integer, that was ignored\")\n",
    "        #    return\n",
    "        assert isFloat(x), f\"x={x} is not a float/float64/numpy.float32/64/128, but a {type(x)}\"\n",
    "        assert np.isnan(x)==False , \"x is NAN\"\n",
    "        assert np.isinf(x)==False , \"x is inf\"\n",
    "        assert np.isinf(-x)==False, \"x is -inf\"\n",
    "        # NOTE: missing test for denormalized float\n",
    "        if checkrange:\n",
    "            z=fabs(x)\n",
    "            assert z>=xmin, f\"abs(x)={z} is smaller that expected min value={xmin}\"\n",
    "            assert z<=xmax, f\"abs(x)={z} is larger that expected max value={xmax}\"\n",
    "        if verbose>0:\n",
    "             print(f\"CheckFloat({x}, type={x} => OK\")\n",
    "\n",
    "# Checks that two 'floats' are 'close' (original from libitmal)\n",
    "def CheckInRange(x, expected, eps=1E-9, autoconverttofloat=True, verbose=0):\n",
    "    assert eps>=0, \"eps is less than zero\"\n",
    "    if autoconverttofloat and (not isFloat(x) or not isFloat(expected) or not isFloat(eps)):\n",
    "        if verbose>1:\n",
    "            print(f\"notice: autoconverting x={x} to float..\")\n",
    "        return CheckInRange(1.0*x, 1.0*expected, 1.0*eps, False, verbose)\n",
    "    CheckFloat(x)\n",
    "    CheckFloat(expected)\n",
    "    CheckFloat(eps)\n",
    "    x0 = expected - eps\n",
    "    x1 = expected + eps\n",
    "    ok = x>=x0 and x<=x1\n",
    "    absdiff = np.fabs(x-expected)\n",
    "    if verbose > 0:\n",
    "        print(f\"CheckInRange(x={x}, expected={expected}, eps={eps}: x in [{x0}; {x1}] => {ok}\")\n",
    "    assert ok, f\"x={x} is not within the range [{x0}; {x1}] for eps={eps}, got eps={absdiff}\"\n",
    "\n",
    "print(\"OK(setup..)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "TODO: implement Omega() here and remove this assert..",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-24461a081b05>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mw_c\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m0.3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mp_a\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOmega\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_a\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[0mp_b\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOmega\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_b\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mp_c\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOmega\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw_c\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-2-24461a081b05>\u001B[0m in \u001B[0;36mOmega\u001B[1;34m(w)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mOmega\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[1;32massert\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"TODO: implement Omega() here and remove this assert..\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m# weight vector format: [w_0 w_1 .. w_d], ie. elem. 0 is the 'bias'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAssertionError\u001B[0m: TODO: implement Omega() here and remove this assert.."
     ]
    }
   ],
   "source": [
    "# TODO: code\n",
    "    \n",
    "def Omega(w):\n",
    "    assert False, \"TODO: implement Omega() here and remove this assert..\"\n",
    " \n",
    "# weight vector format: [w_0 w_1 .. w_d], ie. elem. 0 is the 'bias'    \n",
    "w_a = np.array([1., 2., -3.])  \n",
    "w_b = np.array([1E10, -3E10])\n",
    "w_c = np.array([0.1, 0.2, -0.3, 0])\n",
    "\n",
    "p_a = Omega(w_a)\n",
    "p_b = Omega(w_b)\n",
    "p_c = Omega(w_c)\n",
    "\n",
    "print(f\"P(w0)={p_a}\")\n",
    "print(f\"P(w1)={p_b}\")\n",
    "print(f\"P(w2)={p_c}\")\n",
    "\n",
    "# TEST VECTORS\n",
    "e0 = 2*2+(-3)*(-3)\n",
    "e1 = 9e+20\n",
    "e2 = 0.13\n",
    "\n",
    "CheckInRange(p_a, e0)\n",
    "CheckInRange(p_b, e1)\n",
    "CheckInRange(p_c, e2)\n",
    "\n",
    "print(\"OK\")\n",
    "\n",
    "## Adding Regularization for Linear Regression Models\n",
    "\n",
    "Adding the penalty $\\alpha ||\\bw||^2_2$ actually corresponds to the Scikit-learn model `sklearn.linear_model.Ridge` and there are, as usual, a bewildering array of regulized models to choose from in Scikit-learn with exotic names like `Lasso` and `Lars`\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "\n",
    "Let us just examine `Ridge`, `Lasso` and `ElasticNet` here.\n",
    "\n",
    "### Qb Explain the Ridge Plot\n",
    "\n",
    "First take a peek into the plots (and code) below, that fits the `Ridge`, `Lasso` and `ElasticNet` to a polynomial model. The plots show three fits with different $\\alpha$ values (0, 10$^{-5}$, and 1).\n",
    "\n",
    "First, explain what the different $\\alpha$ does to the actual fitting for the `Ridge` model in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Qb, just run the code..\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, ElasticNet, Lasso\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def FitAndPlotModel(name, model_class, X, X_new, y, **model_kargs):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    alphas=(0, 10**-5, 1) \n",
    "    random_state=42\n",
    "    \n",
    "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
    "        #print(model_kargs)\n",
    "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "        model_pipe = Pipeline([\n",
    "                (\"poly_features\", PolynomialFeatures(degree=12, include_bias=False)),\n",
    "                (\"std_scaler\", StandardScaler()),\n",
    "                (\"regul_reg\", model),\n",
    "            ])\n",
    "            \n",
    "        model_pipe.fit(X, y)\n",
    "        y_new_regul = model_pipe.predict(X_new)\n",
    "        \n",
    "        lw = 2 if alpha > 0 else 1\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "    \n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.title(name)\n",
    "    plt.axis([0, 3, 0, 4])\n",
    "\n",
    "def GenerateData():\n",
    "    np.random.seed(42)\n",
    "    m = 20\n",
    "    X = 3 * np.random.rand(m, 1)\n",
    "    y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "    X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
    "    return X, X_new, y\n",
    "    \n",
    "X, X_new, y = GenerateData()\n",
    "\n",
    "FitAndPlotModel('ridge',      Ridge,        X, X_new, y)\n",
    "FitAndPlotModel('lasso',      Lasso,        X, X_new, y)\n",
    "FitAndPlotModel('elasticnet', ElasticNet,   X, X_new, y, l1_ratio=0.1)\n",
    "\n",
    "print(\"OK(plot)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Explain the math of Ridge, Lasso and ElasticNet..",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-b0b02bee1a15>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# TODO:(in text..)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32massert\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Explain the math of Ridge, Lasso and ElasticNet..\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Explain the math of Ridge, Lasso and ElasticNet.."
     ]
    }
   ],
   "source": [
    "# TODO:(in text..)\n",
    "assert False, \"Explain the math of Ridge, Lasso and ElasticNet..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Regularization and Overfitting\n",
    "\n",
    "Finally, comment on how regularization may be used to reduce a potential tendency to overfit the data\n",
    "\n",
    "Describe the situation with the ___tug-of-war___ between the MSE ($J$) and regulizer ($\\Omega$) terms in $\\tilde{J}$ \n",
    "\n",
    "$$\n",
    "  \\tilde{J} = J + \\alpha \\Omega(\\bw)\\\\\n",
    "$$\n",
    "and the potential problem of $\\bw^*$ being far, far away from the origin, and say for a fixed $\\alpha=1$ in regulizer term (normally for real data $\\alpha \\ll 1$).\n",
    "\n",
    "\n",
    "<img src=\"https://blackboard.au.dk/bbcswebdav/courses/BB-Cou-UUVA-91831/Fildeling/L08/Figs/weights_regularization_l2.png\" alt=\"WARNING: you need to be logged into Blackboard to view images\" style=\"width:240px\">\n",
    "\n",
    "OPTIONAL part: Would data preprocessing in the form of scaling, standardization or normalization be of any help to that particular situation? If so, describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: (in text..)\n",
    "Assert False, \"Explain the tug-of-war..\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}