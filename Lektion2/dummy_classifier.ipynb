{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS||\n",
    "---------||\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0206| CEF, updated and spell checked. \n",
    "2018-0208| CEF, minor text update.\n",
    "2018-0305| CEF, updated with SHN comments.\n",
    "2019-0902| CEF, updated for ITMAL v2.\n",
    "2019-0904| CEF, updated and added conclusion Q.\n",
    "2020-0125| CEF, F20 ITMAL update.\n",
    "2020-0204| CEF, updated page numbers to HOMLv2.\n",
    "\n",
    "## Implementing a dummy classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader from Scikit-learn. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "#### Qa  Load and display the MNIST data\n",
    "\n",
    "There is a `sklearn.datasets.fetch_openml` dataloader interface in Scikit-learn. You can load MNIST data like \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',??) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert at scale (not always needed)\n",
    "#X = X / 255.\n",
    "```\n",
    "\n",
    "but you need to set parameters like `return_X_y` and `cache` if the default values are not suitable! \n",
    "\n",
    "Check out the documentation for the `fetch_openml` MNIST loader, try it out by loading a (X,y) MNIST data set, and plot a single digit via the `MNIST_PlotDigit` function here (input data is a 28x28 NMIST subimage)\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "```\n",
    "\n",
    "Finally, put the MNIST loader into a single function called `MNIST_GetDataSet()` so you can resuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "\n",
    "def MNIST_GetDataSet():\n",
    "    # Load data from https://www.openml.org/d/554\n",
    "    X, y = fetch_openml('mnist_784', return_X_y=True)  \n",
    "    return X, y\n",
    "\n",
    "X, y =MNIST_GetDataSet()\n",
    "\n",
    "MNIST_PlotDigit(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOML], p.88.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.(We will be looking at cross-validation instead of the simple fit-predict in a later exercise.)\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader...\n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(70000, 784)\n",
      "X.shape=(70000, 784)\n",
      "X_train.shape=(56000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#X, y = MNIST_GetDataSet()\n",
    "\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "print(f\"X_train.shape={X_train.shape}\")\n",
    "y_train_5 = (y_train == '5')  \n",
    "y_test_5 = (y_test == '5')  \n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4    13 ... 13974 13995 13996]\n",
      "[  158   215   225   326   403   432   517   594   607   828   875  1027\n",
      "  1221  1351  1399  1442  1525  1614  1617  1637  1642  1644  1696  1726\n",
      "  1805  1847  1880  1956  2052  2171  2212  2252  2380  2405  2552  2593\n",
      "  2768  2770  2893  2912  2957  2975  2985  3078  3114  3286  3295  3344\n",
      "  3682  3709  3788  3893  3911  3939  4023  4093  4111  4145  4264  4346\n",
      "  4357  4388  4390  4397  4404  4407  4410  4535  4567  4604  4633  4759\n",
      "  4785  4857  5202  5228  5312  5313  5331  5384  5567  5574  5578  5580\n",
      "  5607  5623  5767  5798  5868  5953  5980  6038  6092  6205  6310  6341\n",
      "  6359  6381  6635  6683  6839  6867  6875  6990  7039  7109  7150  7166\n",
      "  7279  7373  7667  7750  7789  7806  7968  8012  8126  8181  8261  8524\n",
      "  8529  8638  8654  8753  8798  8848  8929  8982  8997  9162  9181  9217\n",
      "  9240  9389  9417  9458  9545  9611  9722  9877  9930 10060 10189 10282\n",
      " 10382 10395 10494 10665 10691 10768 10952 10959 10971 11077 11080 11091\n",
      " 11195 11273 11365 11401 11483 11497 11610 11736 11775 11907 11947 12020\n",
      " 12075 12154 12311 12389 12569 12739 12775 12825 12862 12874 12970 12979\n",
      " 12982 13021 13079 13258 13332 13335 13419 13421 13461 13483 13592 13603\n",
      " 13732 13773 13837 13884 13898]\n"
     ]
    }
   ],
   "source": [
    "#Find true positive & false negatives \n",
    "import numpy as np\n",
    "\n",
    "y_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "\n",
    "#Calcuating true positives & print them\n",
    "y_pred_positive = np.where(y_pred == True)\n",
    "y_test_positive = np.where(y_test == '5')\n",
    "y_true_positive = y_pred_positive and y_test_positive \n",
    "y_true_positive = y_true_positive[0] # The first element of this array contains the indices with true positives\n",
    "print(y_true_positive) \n",
    "\n",
    "#Calcuating fales negatives & print them\n",
    "y_pred_negative = np.where(y_pred == False)\n",
    "y_test_negative = np.where(y_test != '5')\n",
    "y_false_negative = np.setdiff1d(y_pred_negative, y_test_negative)\n",
    "\n",
    "print(y_false_negative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We can now plot misclassified numbers\n",
    "#True positive number\n",
    "MNIST_PlotDigit(X_test[y_true_positive[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#False negative number\n",
    "MNIST_PlotDigit(X_test[y_false_negative[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc Implement a dummy binary classifier\n",
    "\n",
    "Now we will try to create a Scikit-learn compatible estimator implemented via a python class. Follow the code found in [HOML], p.90, but name you estimator `DummyClassifier` instead of `Never5Classifyer`.\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/F20_itmal/L02/Figs/class_base_estimator.png\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit from `BaseEstimator` (and possibly also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duck typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and find a way to extract the accuracy `score` from the test data. You may implement an accuracy function yourself or just use the `sklearn.metrics.accuracy_score` function. \n",
    "\n",
    "Finally, compare the accuracy score from your `DummyClassifier` with the scores found in [HOML] \"Measuring Accuracy Using Cross-Validation\", p.89. Are they comparable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class DummyClassifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "    def get_params():\n",
    "        pass\n",
    "\n",
    "dummy_clf = DummyClassifier()\n",
    "dummy_clf.fit(X, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True ... False False False]\n",
      "0.9129285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test_5,y_pred)\n",
    "\n",
    "print(y_test_5)\n",
    "\n",
    "print(score)\n",
    "# The accuracy is very similar because we are always guessing that a number is never a 5, \n",
    "# therefore about 90% of the time we are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We learn how to fetch a dataset using scikitlearn. We use this MNIST dataset to learn how to do a binary classification\n",
    "# with a stochastic descent gradient model. We then learn how to split a dataset in python into a training & test set, which \n",
    "# can use to train our model. \n",
    "# After training our model we can find various parameters to judge our model, and we find that even though we may have a lot\n",
    "# of true positive (Correct classification of the number 5) it does not necessarily mean our model is good. We need to hold \n",
    "# compare the amount of true positive to true negative, false positive and negative to get an accurate idea of whether our \n",
    "# model is useful for classifying our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
